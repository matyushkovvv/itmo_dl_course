## 1. Загрузка и подготовка данных

Используется стандартный подход:

1. pandas для чтения CSV
2. удаление столбцов id и целевой переменной smoking
3. стандартизация признаков через StandardScaler

Без нормализации обучение будет нестабильным.

Разбиение со stratify=y, что сохраняет баланс классов.

## 2. Перевод данных в PyTorch

``` python
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
```

Используется float-таргет для последующего применения Focal Loss

## 3. Focal Loss вместо обычной BCE

Реализована кастомная функция потерь:

усиливает вклад сложных примеров
уменьшает влияние лёгких
полезно при дисбалансе классов

Формула соответствует классической статье Lin et al.

Параметры: - gamma=2 стандартное значение - alpha=0.75 увеличивает вклад положительного класса

## 4. Архитектура нейросети

256 → 512 → 512 → 256 → 128 → 64 → 1

Используются:

BatchNorm после почти каждого слоя
Dropout (0.1-0.2)
ReLU активации

Плюсы: - регуляризация и устойчивость обучения

Минусы: - риск переобучения при малом
датасете

## 5. Оптимизация

``` python
optimizer = AdamW(lr=1e-3, weight_decay=1e-4)
```
AdamW лучше контролирует переобучение
weight decay как L2 регуляризация


## 6. Валидация

Используется:

сигмоида
порог 0.5
метрика accuracy
